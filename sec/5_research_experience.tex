
\begin{rSection}{Research Experience}
    
     \small{{\bf On the Adversarial Implications of Attention Sinks in LLMs}  \hfill Apr. 2025 - Present
    \\ Supervisor: \href{https://lsjxjtu.github.io/index.html}{\textcolor{darkblue}{Prof. Sijia Liu}} (MSU)
\vspace{-1.5mm}
    
    % Topics: Machine Unlearning, RLHF 
    \quad $\bullet$ Investigating attention sinks in LLMs to develop more effective backdoor poisoning attacks.
\vspace{-2mm}
    
    \quad $\bullet$ Exploring applications in unlearned models where backdoor triggers can selectively recover forgotten knowledge.

    }
    
    \small{{\bf Privacy-Preserving Tuning for Large Models}  \hfill Dec. 2023 - Mar. 2025
    \\  Supervisors: \href{https://scholar.google.com/citations?user=TN09YMcAAAAJ&hl=en}{\textcolor{darkblue}{Prof. Qi Zhu}} (NU), \ \href{https://wangxiao1254.github.io/}{\textcolor{darkblue}{Prof. Xiao Wang}} (NU)
\vspace{-1.5mm}
    
    
    \quad $\bullet$ Developed Split Adaptation (SA) to ensure \textbf{data privacy }during adaptation of pre-trained Vision Transformers
(ViTs), utilizing bi-level noise injection for privacy-preserving downstream tasks without data sharing.
\vspace{-2mm}

    \quad $\bullet$ Protected \textbf{model privacy} by sharing only a low-bit quantized frontend of the ViT, preventing model leakage and
ensuring secure adaptation.
\vspace{-2mm}

    \quad $\bullet$ \textbf{Publication}: \pubcite{1}


}
    \end{rSection}