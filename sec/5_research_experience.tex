
\begin{rSection}{Research Experience}
    
     \small{{\bf Backdooring Machine Unlearning via Attention Sinks}  \hfill Apr. 2025 - Oct. 2025
    \\ Supervisor: \href{https://lsjxjtu.github.io/index.html}{\textcolor{BS_color}{Prof. Sijia Liu}} (MSU)
\vspace{-1.5mm}
    
    % Topics: Machine Unlearning, RLHF 
    \quad $\bullet$ Exposed a vulnerability in LLM unlearning: attention-sinkâ€“guided backdoors can \textbf{reactivate} forgotten behaviors.
\vspace{-2mm}
    
    \quad $\bullet$ Exploring applications in unlearned models where backdoor triggers can selectively recover forgotten knowledge.

    \quad $\bullet$ \textbf{Publication}: \pubcite{1}.
    

    }
    
    \small{{\bf Privacy-Preserving Tuning for Large Models}  \hfill Dec. 2023 - Mar. 2025
    \\  Supervisors: \href{https://scholar.google.com/citations?user=TN09YMcAAAAJ&hl=en}{\textcolor{BS_color}{Prof. Qi Zhu}} (NU), \ \href{https://wangxiao1254.github.io/}{\textcolor{BS_color}{Prof. Xiao Wang}} (NU)
\vspace{-1.5mm}
    
    
    \quad $\bullet$ Developed Split Adaptation (SA) to ensure \textbf{data privacy }during adaptation of pre-trained Vision Transformers
(ViTs), utilizing bi-level noise injection for privacy-preserving downstream tasks without data sharing.
\vspace{-2mm}

    \quad $\bullet$ Protected \textbf{model privacy} by sharing only a low-bit quantized frontend of the ViT, preventing model leakage and
ensuring secure adaptation.
\vspace{-2mm}

    \quad $\bullet$ \textbf{Publication}: \pubcite{2}.


}
    \end{rSection}